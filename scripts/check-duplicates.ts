#!/usr/bin/env tsx

/**
 * ä»£ç é‡å¤æ£€æµ‹è„šæœ¬
 * è‡ªåŠ¨æ£€æµ‹é¡¹ç›®ä¸­çš„é‡å¤ä»£ç æ¨¡å¼ï¼Œå¸®åŠ©ç»´æŠ¤ä»£ç è´¨é‡
 */

import { execSync } from 'child_process'
import { existsSync, readFileSync, readdirSync, statSync } from 'fs'
import { join } from 'path'
import { createHash } from 'crypto'

interface DuplicatePattern {
  pattern: string
  files: string[]
  lines: number
  hash: string
}

interface DuplicateReport {
  totalFiles: number
  duplicatePatterns: DuplicatePattern[]
  summary: {
    totalDuplicates: number
    totalLinesAffected: number
    severityHigh: number
    severityMedium: number
    severityLow: number
  }
}

class DuplicateDetector {
  private readonly projectRoot: string
  private readonly excludePatterns: string[] = [
    'node_modules',
    '.git',
    'target',
    'dist',
    'build',
    '.cache',
    'coverage',
    '.next',
    '.nuxt',
    '.output'
  ]

  private readonly includeExtensions: string[] = [
    '.ts', '.tsx', '.js', '.jsx', '.rs', '.json', '.yaml', '.yml'
  ]

  constructor(projectRoot: string) {
    this.projectRoot = projectRoot
  }

  /**
   * æ£€æµ‹ç¯å¢ƒæ£€æµ‹ä»£ç æ¨¡å¼
   */
  private detectEnvironmentChecks(): DuplicatePattern[] {
    const patterns: DuplicatePattern[] = []
    
    // æ£€æŸ¥é—ç•™çš„ç¯å¢ƒæ£€æµ‹ä»£ç 
    const envCheckPattern = /typeof window !== 'undefined' && '__TAURI__' in window/g
    const files = this.findFilesWithPattern(envCheckPattern)
    
    if (files.length > 1) {
      patterns.push({
        pattern: 'Environment detection code (isInTauri)',
        files,
        lines: files.length,
        hash: this.generateHash('env-check')
      })
    }

    return patterns
  }

  /**
   * æ£€æµ‹é‡å¤çš„å¯¼å…¥è¯­å¥
   */
  private detectDuplicateImports(): DuplicatePattern[] {
    const patterns: DuplicatePattern[] = []
    const importMap = new Map<string, string[]>()

    const files = this.getAllSourceFiles()
    
    for (const file of files) {
      try {
        const content = readFileSync(file, 'utf-8')
        const imports = content.match(/^import.*from.*$/gm) || []
        
        for (const importLine of imports) {
          const normalized = importLine.trim()
          if (!importMap.has(normalized)) {
            importMap.set(normalized, [])
          }
          importMap.get(normalized)!.push(file)
        }
      } catch (error) {
        // Skip files that can't be read
      }
    }

    // Find imports that appear in multiple files
    for (const [importLine, fileList] of importMap.entries()) {
      if (fileList.length > 3) { // Only report if appears in 4+ files
        patterns.push({
          pattern: `Duplicate import: ${importLine}`,
          files: fileList,
          lines: fileList.length,
          hash: this.generateHash(importLine)
        })
      }
    }

    return patterns
  }

  /**
   * æ£€æµ‹é‡å¤çš„ç±»å‹å®šä¹‰
   */
  private detectDuplicateTypes(): DuplicatePattern[] {
    const patterns: DuplicatePattern[] = []
    const typeMap = new Map<string, string[]>()

    const files = this.getAllSourceFiles().filter(f => f.endsWith('.ts') || f.endsWith('.tsx'))
    
    for (const file of files) {
      try {
        const content = readFileSync(file, 'utf-8')
        const types = content.match(/^(export\s+)?(interface|type|enum)\s+\w+/gm) || []
        
        for (const typeLine of types) {
          const normalized = typeLine.replace(/^export\s+/, '').trim()
          if (!typeMap.has(normalized)) {
            typeMap.set(normalized, [])
          }
          typeMap.get(normalized)!.push(file)
        }
      } catch (error) {
        // Skip files that can't be read
      }
    }

    // Find types that appear in multiple files
    for (const [typeLine, fileList] of typeMap.entries()) {
      if (fileList.length > 1) {
        patterns.push({
          pattern: `Duplicate type definition: ${typeLine}`,
          files: fileList,
          lines: fileList.length,
          hash: this.generateHash(typeLine)
        })
      }
    }

    return patterns
  }

  /**
   * æ£€æµ‹é‡å¤çš„é…ç½®æ–‡ä»¶å†…å®¹
   */
  private detectDuplicateConfigs(): DuplicatePattern[] {
    const patterns: DuplicatePattern[] = []
    const configFiles = this.getAllSourceFiles().filter(f => 
      f.includes('config') || f.endsWith('.json') || f.endsWith('.yml') || f.endsWith('.yaml')
    )

    const contentMap = new Map<string, string[]>()

    for (const file of configFiles) {
      try {
        const content = readFileSync(file, 'utf-8')
        const hash = this.generateHash(content)
        
        if (!contentMap.has(hash)) {
          contentMap.set(hash, [])
        }
        contentMap.get(hash)!.push(file)
      } catch (error) {
        // Skip files that can't be read
      }
    }

    // Find identical config files
    for (const [hash, fileList] of contentMap.entries()) {
      if (fileList.length > 1) {
        patterns.push({
          pattern: 'Identical configuration files',
          files: fileList,
          lines: fileList.length,
          hash
        })
      }
    }

    return patterns
  }

  /**
   * æŸ¥æ‰¾åŒ…å«ç‰¹å®šæ¨¡å¼çš„æ–‡ä»¶
   */
  private findFilesWithPattern(pattern: RegExp): string[] {
    const files: string[] = []
    const sourceFiles = this.getAllSourceFiles()

    for (const file of sourceFiles) {
      try {
        const content = readFileSync(file, 'utf-8')
        if (pattern.test(content)) {
          files.push(file)
        }
      } catch (error) {
        // Skip files that can't be read
      }
    }

    return files
  }

  /**
   * è·å–æ‰€æœ‰æºæ–‡ä»¶
   */
  private getAllSourceFiles(): string[] {
    const files: string[] = []
    
    const traverse = (dir: string): void => {
      try {
        const entries = readdirSync(dir)
        
        for (const entry of entries) {
          const fullPath = join(dir, entry)
          
          if (this.excludePatterns.some(pattern => fullPath.includes(pattern))) {
            continue
          }
          
          const stat = statSync(fullPath)
          
          if (stat.isDirectory()) {
            traverse(fullPath)
          } else if (this.includeExtensions.some(ext => fullPath.endsWith(ext))) {
            files.push(fullPath)
          }
        }
      } catch (error) {
        // Skip directories that can't be read
      }
    }

    traverse(this.projectRoot)
    return files
  }

  /**
   * ç”Ÿæˆå†…å®¹å“ˆå¸Œ
   */
  private generateHash(content: string): string {
    return createHash('md5').update(content).digest('hex').substring(0, 8)
  }

  /**
   * æ‰§è¡Œå®Œæ•´çš„é‡å¤æ£€æµ‹
   */
  public detect(): DuplicateReport {
    console.log('ğŸ” Starting duplicate code detection...')
    
    const allPatterns: DuplicatePattern[] = [
      ...this.detectEnvironmentChecks(),
      ...this.detectDuplicateImports(),
      ...this.detectDuplicateTypes(),
      ...this.detectDuplicateConfigs(),
    ]

    const totalFiles = this.getAllSourceFiles().length
    const totalLinesAffected = allPatterns.reduce((sum, p) => sum + p.lines, 0)

    // Classify severity
    let severityHigh = 0
    let severityMedium = 0
    let severityLow = 0

    for (const pattern of allPatterns) {
      if (pattern.files.length >= 10) {
        severityHigh++
      } else if (pattern.files.length >= 5) {
        severityMedium++
      } else {
        severityLow++
      }
    }

    return {
      totalFiles,
      duplicatePatterns: allPatterns,
      summary: {
        totalDuplicates: allPatterns.length,
        totalLinesAffected,
        severityHigh,
        severityMedium,
        severityLow
      }
    }
  }

  /**
   * ç”ŸæˆæŠ¥å‘Š
   */
  public generateReport(report: DuplicateReport): string {
    let output = '\nğŸ¯ Duplicate Code Detection Report\n'
    output += '=' .repeat(50) + '\n\n'

    output += `ğŸ“Š Summary:\n`
    output += `  â€¢ Total files scanned: ${report.totalFiles}\n`
    output += `  â€¢ Duplicate patterns found: ${report.summary.totalDuplicates}\n`
    output += `  â€¢ Total lines affected: ${report.summary.totalLinesAffected}\n`
    output += `  â€¢ High severity (10+ occurrences): ${report.summary.severityHigh}\n`
    output += `  â€¢ Medium severity (5-9 occurrences): ${report.summary.severityMedium}\n`
    output += `  â€¢ Low severity (2-4 occurrences): ${report.summary.severityLow}\n\n`

    if (report.duplicatePatterns.length === 0) {
      output += 'âœ… No significant duplicate code patterns detected!\n\n'
      return output
    }

    output += 'ğŸ” Detected Patterns:\n\n'

    for (const pattern of report.duplicatePatterns) {
      const severity = pattern.files.length >= 10 ? 'ğŸš¨ HIGH' : 
                      pattern.files.length >= 5 ? 'âš ï¸ MEDIUM' : 'â„¹ï¸ LOW'
      
      output += `${severity} ${pattern.pattern} (${pattern.files.length} occurrences)\n`
      output += `  Hash: ${pattern.hash}\n`
      output += `  Files:\n`
      
      for (const file of pattern.files.slice(0, 10)) { // Show max 10 files
        output += `    â€¢ ${file.replace(this.projectRoot, '.')}\n`
      }
      
      if (pattern.files.length > 10) {
        output += `    â€¢ ... and ${pattern.files.length - 10} more files\n`
      }
      
      output += '\n'
    }

    return output
  }
}

// Main execution
if (import.meta.url === `file://${process.argv[1]}`) {
  const projectRoot = process.cwd()
  const detector = new DuplicateDetector(projectRoot)
  
  const report = detector.detect()
  const reportText = detector.generateReport(report)
  
  console.log(reportText)
  
  // Exit with error code if duplicates found
  if (report.summary.totalDuplicates > 0) {
    process.exit(1)
  }
}
